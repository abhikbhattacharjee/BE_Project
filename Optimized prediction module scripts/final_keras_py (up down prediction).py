# -*- coding: utf-8 -*-
"""Final_keras.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FySglYkS3at-eLduKohPgZ4jjLKJxZ3d
"""

import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn import datasets
import numpy as np

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_excel(io.BytesIO(uploaded['up_down_only.xlsx']), sep='\t', encoding = 'utf-8')
df = pd.DataFrame(df)

"""Equal up and down observations code, ONLY if necessary"""

df_down = df.iloc[0:93,]
#df_not_diff = df.iloc[93:351,].sample(n=93)
df_up = df.iloc[93:223,].sample(n=93)
df_93_counts = pd.concat([df_down, df_up]).sample(frac=1)
df=df_93_counts

"""93 counts code ends"""

df = df.sample(frac=1)

'''Shit Held code to remove one feature'''
try:
  df = df.drop(columns='denovo_ubx')
except:
  print ("Not Permissible")

"""Execute only to pass complete dataset"""

A = df.iloc[:,1:49]
B = df.iloc[:,49]

encoder = LabelEncoder()
encoder.fit(B)
encoded_b = encoder.transform(B)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_b = np_utils.to_categorical(encoded_b)

"""Complete dataset part ends"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

"""Stratified split, balanced split"""

InputX = df.iloc[:,1:49]
InputY = df.iloc[:,49]

A1, A2, B1, B2 = train_test_split(InputX, InputY, test_size=0.25, stratify=InputY)

encoder = LabelEncoder()
encoder.fit(B1)
encoded_b1 = encoder.transform(B1)
dummy_b1 = np_utils.to_categorical(encoded_b1)

"""Stratified split ends

*Unstratified* split starts
"""

'''train1, test1 = train_test_split(df, test_size=0.25)

A1 = train1.iloc[:,1:49]
B1 = train1.iloc[:,49]
A2 = test1.iloc[:,1:49]
B2 = test1.iloc[:,49]


encoder = LabelEncoder()
encoder.fit(B1)
encoded_b1 = encoder.transform(B1)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_b1 = np_utils.to_categorical(encoded_b1)
'''

"""Unstratified split ends"""

'''model = Sequential()
model.add(Dense(43, input_dim=48, activation='relu'))
model.add(Dense(38))
model.add(Dense(33))
model.add(Dense(28))
model.add(Dense(23))
model.add(Dense(18))
model.add(Dense(13))
model.add(Dense(8))
model.add(Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
test = model.fit(A1,dummy_b1, epochs = 50, batch_size = 5)'''

"""Hyperparameters adjustment based on thumb rules"""

model = Sequential()
model.add(Dense(43, input_dim=48, activation='relu'))
model.add(Dense(16))
model.add(Dense(8))
model.add(Dense(4))
model.add(Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
test = model.fit(A,dummy_b, epochs = 150, batch_size = 110) #batch size 10 optimal

dff = pd.DataFrame(model.predict(A2))
dff.index = A2.index
#dff.rename(columns={0:'Down',1:'Not_diff',2:'up'}, inplace=True)
dff.rename(columns={0:'Down',1:'up'}, inplace=True)
result = dff.idxmax(axis=1)
result.index = dff.index

"""Removal of ambiguous results"""

#print(dff[abs(dff.diff(axis=1)['up'])>0.4])
b2df = B2[abs(dff.diff(axis=1)['up'])>0.1]
r1 = result[abs(dff.diff(axis=1)['up'])>0.1]

print('Removed ambigious results')
print(confusion_matrix(b2df, r1))
print(classification_report(b2df, r1))

print('All results')
print(confusion_matrix(B2, result))
print(classification_report(B2, result))

"""################
> Tracking Validation loss via plots
"""

import matplotlib.pyplot as plt
plt.plot(test.history['acc'])
plt.plot(test.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(test.history['loss'])
plt.plot(test.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Split for 93 counts"""

df_down = df.iloc[0:93,]
df_not_diff = df.iloc[93:351,].sample(n=93)
df_up = df.iloc[351:480,].sample(n=93)
df_93_counts = pd.concat([df_down, df_not_diff, df_up]).sample(frac=1)
df=df_93_counts