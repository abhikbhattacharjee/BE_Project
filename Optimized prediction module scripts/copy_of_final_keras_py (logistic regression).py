# -*- coding: utf-8 -*-
"""Copy of Final_keras.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zm7O7NTkAT18GZ0PIdsOhl4uPyF6waiZ
"""

import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn import datasets

!pip install eli5

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_excel(io.BytesIO(uploaded['keras_train_data.xlsx']), sep='\t', encoding = 'utf-8')

df = df.sample(frac=1)
df

#A = df_93_counts.iloc[:,1:49]
#B = df_93_counts.iloc[:,49]

A = df.iloc[:,1:49]
B = df.iloc[:,49]

encoder = LabelEncoder()
encoder.fit(B)
encoded_b = encoder.transform(B)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_b = np_utils.to_categorical(encoded_b)

"""Sparse Logisitic Regression Implementation"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

train1, test1 = train_test_split(df, test_size=0.25)
scaler = StandardScaler()
#train1 = scaler.transform(train1)
#test1 = scaler.transform(test1)

A1 = train1.iloc[:,1:49]
B1 = train1.iloc[:,49]
A2 = test1.iloc[:,1:49]
B2 = test1.iloc[:,49]

#A1 = scaler.fit_transform(A1)
#A2 = scaler.fit_transform(A2)

encoder = LabelEncoder()
encoder.fit(B1)
encoded_b1 = encoder.transform(B1)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_b1 = np_utils.to_categorical(encoded_b1)

A1

from sklearn.linear_model import LogisticRegression
logit = LogisticRegression(solver = 'lbfgs')
logit.fit(A1,B1)

target_names = list(df.columns.drop(['labels','sr no']))
target_names

import eli5
eli5.show_weights(logit, feature_names = target_names, target_names=['Down', 'up'], top=50)

"""Logistic Regression Ends"""

model = Sequential()
model.add(Dense(43, input_dim=48, activation='relu'))
model.add(Dense(38))
model.add(Dense(33))
model.add(Dense(28))
model.add(Dense(23))
model.add(Dense(18))
model.add(Dense(13))
model.add(Dense(8))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
test = model.fit(A,dummy_b,validation_split=0.25, epochs = 15, batch_size = 5)

import matplotlib.pyplot as plt
# summarize history for accuracy
plt.plot(test.history['acc'])
plt.plot(test.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(test.history['loss'])
plt.plot(test.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

ynew = model.predict(A2)
dff = pd.DataFrame(ynew)
dff.rename(columns={0:'Down',1:'Not_diff',2:'up'}, inplace=True)
result = dff.idxmax(axis=1)

print(confusion_matrix(B2, result))
print(classification_report(B2, result))





df_down = df.iloc[0:93,]
df_not_diff = df.iloc[93:351,].sample(n=93)
df_up = df.iloc[351:480,].sample(n=93)
df_93_counts = pd.concat([df_down, df_not_diff, df_up]).sample(frac=1)
df_93_counts

